{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1704b261-c75a-43d9-bd70-bf7305333e77",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "ac37cb99-4996-4f08-8ed0-ae79fc95855c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#precision is really important here \n",
    "#precision is number of true positives over number of predicted positives\n",
    "\n",
    "#Interpretation: High precision means the model is good at minimizing false positives—it \n",
    "#doesn’t falsely label a negative as a positive (which overlaps with specificity).\n",
    "\n",
    "#(true pos)/ ("
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "d1023b52-0951-4870-9929-c64e7fe0affe",
   "metadata": {},
   "outputs": [],
   "source": [
    "#specificity is actaully the most important here (true negative rate)\n",
    "\n",
    "#calculated by (true negative)/ (true negative + false positives)\n",
    "\n",
    "#Specificity is the proportion of actual negatives that are correctly identified by the model as negative. \n",
    "#In other words, it measures the model's ability to correctly identify the negative class (i.e., the true negatives).\n",
    "\n",
    "#Interpretation: High specificity means the model is good at avoiding false positives (it is not incorrectly\n",
    "#classifying negatives as positives).\n",
    "\n",
    "#Specificity refers to how well the model avoids false positives and correctly classifies negative cases (TN).\n",
    "\n",
    "#If you say that specificity is more important, you're emphasizing the importance of minimizing false \n",
    "#positives and ensuring that the negative class is correctly identified. This is not the same as precision or recall:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "29137d57-3bc5-4282-a79b-8f3ad00497c4",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "EOF while scanning triple-quoted string literal (580510921.py, line 7)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[49], line 7\u001b[0;36m\u001b[0m\n\u001b[0;31m    5) combine the the two to make your new train set\u001b[0m\n\u001b[0m                                                     \n^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m EOF while scanning triple-quoted string literal\n"
     ]
    }
   ],
   "source": [
    "#should try to equalize the data proportions through SMOTE and undersampling\n",
    "\"\"\"\n",
    "1) find size of train set \n",
    "2) find existing proprtion of target beans\n",
    "3) smote enough to bring this up to around 50% of the train size\n",
    "4) randomly undersample the negatives to select roughly the other 50%\n",
    "5) combine the the two to make your new train set\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e350b3fa-99c4-4d31-9f02-9b2099e1e919",
   "metadata": {},
   "outputs": [],
   "source": [
    "def precision(y, y_hat):\n",
    "    # Could also use confusion matrix\n",
    "    y_y_hat = list(zip(y, y_hat))\n",
    "    tp = sum([1 for i in y_y_hat if i[0] == 1 and i[1] == 1])\n",
    "    fp = sum([1 for i in y_y_hat if i[0] == 0 and i[1] == 1])\n",
    "    return tp / float(tp + fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6f29bbbe-c927-437d-8d4c-0a5944548ce5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a746bf7e-f856-4d3a-bc78-e48e2b824a0d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Area</th>\n",
       "      <th>Perimeter</th>\n",
       "      <th>MajorAxisLength</th>\n",
       "      <th>MinorAxisLength</th>\n",
       "      <th>AspectRation</th>\n",
       "      <th>Eccentricity</th>\n",
       "      <th>ConvexArea</th>\n",
       "      <th>EquivDiameter</th>\n",
       "      <th>Extent</th>\n",
       "      <th>Solidity</th>\n",
       "      <th>roundness</th>\n",
       "      <th>Compactness</th>\n",
       "      <th>ShapeFactor1</th>\n",
       "      <th>ShapeFactor2</th>\n",
       "      <th>ShapeFactor3</th>\n",
       "      <th>ShapeFactor4</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>28395</td>\n",
       "      <td>610.291</td>\n",
       "      <td>208.178117</td>\n",
       "      <td>173.888747</td>\n",
       "      <td>1.197191</td>\n",
       "      <td>0.549812</td>\n",
       "      <td>28715</td>\n",
       "      <td>190.141097</td>\n",
       "      <td>0.763923</td>\n",
       "      <td>0.988856</td>\n",
       "      <td>0.958027</td>\n",
       "      <td>0.913358</td>\n",
       "      <td>0.007332</td>\n",
       "      <td>0.003147</td>\n",
       "      <td>0.834222</td>\n",
       "      <td>0.998724</td>\n",
       "      <td>SEKER</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>28734</td>\n",
       "      <td>638.018</td>\n",
       "      <td>200.524796</td>\n",
       "      <td>182.734419</td>\n",
       "      <td>1.097356</td>\n",
       "      <td>0.411785</td>\n",
       "      <td>29172</td>\n",
       "      <td>191.272750</td>\n",
       "      <td>0.783968</td>\n",
       "      <td>0.984986</td>\n",
       "      <td>0.887034</td>\n",
       "      <td>0.953861</td>\n",
       "      <td>0.006979</td>\n",
       "      <td>0.003564</td>\n",
       "      <td>0.909851</td>\n",
       "      <td>0.998430</td>\n",
       "      <td>SEKER</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>29380</td>\n",
       "      <td>624.110</td>\n",
       "      <td>212.826130</td>\n",
       "      <td>175.931143</td>\n",
       "      <td>1.209713</td>\n",
       "      <td>0.562727</td>\n",
       "      <td>29690</td>\n",
       "      <td>193.410904</td>\n",
       "      <td>0.778113</td>\n",
       "      <td>0.989559</td>\n",
       "      <td>0.947849</td>\n",
       "      <td>0.908774</td>\n",
       "      <td>0.007244</td>\n",
       "      <td>0.003048</td>\n",
       "      <td>0.825871</td>\n",
       "      <td>0.999066</td>\n",
       "      <td>SEKER</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>30008</td>\n",
       "      <td>645.884</td>\n",
       "      <td>210.557999</td>\n",
       "      <td>182.516516</td>\n",
       "      <td>1.153638</td>\n",
       "      <td>0.498616</td>\n",
       "      <td>30724</td>\n",
       "      <td>195.467062</td>\n",
       "      <td>0.782681</td>\n",
       "      <td>0.976696</td>\n",
       "      <td>0.903936</td>\n",
       "      <td>0.928329</td>\n",
       "      <td>0.007017</td>\n",
       "      <td>0.003215</td>\n",
       "      <td>0.861794</td>\n",
       "      <td>0.994199</td>\n",
       "      <td>SEKER</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>30140</td>\n",
       "      <td>620.134</td>\n",
       "      <td>201.847882</td>\n",
       "      <td>190.279279</td>\n",
       "      <td>1.060798</td>\n",
       "      <td>0.333680</td>\n",
       "      <td>30417</td>\n",
       "      <td>195.896503</td>\n",
       "      <td>0.773098</td>\n",
       "      <td>0.990893</td>\n",
       "      <td>0.984877</td>\n",
       "      <td>0.970516</td>\n",
       "      <td>0.006697</td>\n",
       "      <td>0.003665</td>\n",
       "      <td>0.941900</td>\n",
       "      <td>0.999166</td>\n",
       "      <td>SEKER</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Area  Perimeter  MajorAxisLength  MinorAxisLength  AspectRation  \\\n",
       "0  28395    610.291       208.178117       173.888747      1.197191   \n",
       "1  28734    638.018       200.524796       182.734419      1.097356   \n",
       "2  29380    624.110       212.826130       175.931143      1.209713   \n",
       "3  30008    645.884       210.557999       182.516516      1.153638   \n",
       "4  30140    620.134       201.847882       190.279279      1.060798   \n",
       "\n",
       "   Eccentricity  ConvexArea  EquivDiameter    Extent  Solidity  roundness  \\\n",
       "0      0.549812       28715     190.141097  0.763923  0.988856   0.958027   \n",
       "1      0.411785       29172     191.272750  0.783968  0.984986   0.887034   \n",
       "2      0.562727       29690     193.410904  0.778113  0.989559   0.947849   \n",
       "3      0.498616       30724     195.467062  0.782681  0.976696   0.903936   \n",
       "4      0.333680       30417     195.896503  0.773098  0.990893   0.984877   \n",
       "\n",
       "   Compactness  ShapeFactor1  ShapeFactor2  ShapeFactor3  ShapeFactor4  Class  \n",
       "0     0.913358      0.007332      0.003147      0.834222      0.998724  SEKER  \n",
       "1     0.953861      0.006979      0.003564      0.909851      0.998430  SEKER  \n",
       "2     0.908774      0.007244      0.003048      0.825871      0.999066  SEKER  \n",
       "3     0.928329      0.007017      0.003215      0.861794      0.994199  SEKER  \n",
       "4     0.970516      0.006697      0.003665      0.941900      0.999166  SEKER  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bean_df= pd.read_excel('my_data/Dry_Bean_Dataset.xlsx')\n",
    "bean_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0778fe48-80d4-42f5-a363-49b4268f0c43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 13611 entries, 0 to 13610\n",
      "Data columns (total 17 columns):\n",
      " #   Column           Non-Null Count  Dtype  \n",
      "---  ------           --------------  -----  \n",
      " 0   Area             13611 non-null  int64  \n",
      " 1   Perimeter        13611 non-null  float64\n",
      " 2   MajorAxisLength  13611 non-null  float64\n",
      " 3   MinorAxisLength  13611 non-null  float64\n",
      " 4   AspectRation     13611 non-null  float64\n",
      " 5   Eccentricity     13611 non-null  float64\n",
      " 6   ConvexArea       13611 non-null  int64  \n",
      " 7   EquivDiameter    13611 non-null  float64\n",
      " 8   Extent           13611 non-null  float64\n",
      " 9   Solidity         13611 non-null  float64\n",
      " 10  roundness        13611 non-null  float64\n",
      " 11  Compactness      13611 non-null  float64\n",
      " 12  ShapeFactor1     13611 non-null  float64\n",
      " 13  ShapeFactor2     13611 non-null  float64\n",
      " 14  ShapeFactor3     13611 non-null  float64\n",
      " 15  ShapeFactor4     13611 non-null  float64\n",
      " 16  Class            13611 non-null  object \n",
      "dtypes: float64(14), int64(2), object(1)\n",
      "memory usage: 1.8+ MB\n"
     ]
    }
   ],
   "source": [
    "bean_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1770bac7-7f68-49fc-b7f2-faa61a8cd1b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n1.) Area (A): The area of a bean zone and the number of pixels within its boundaries.\\n2.) Perimeter (P): Bean circumference is defined as the length of its border.\\n3.) Major axis length (L): The distance between the ends of the longest line that can be drawn from a bean.\\n4.) Minor axis length (l): The longest line that can be drawn from the bean while standing perpendicular to the main axis.\\n5.) Aspect ratio (K): Defines the relationship between L and l.\\n6.) Eccentricity (Ec): Eccentricity of the ellipse having the same moments as the region.\\n7.) Convex area (C): Number of pixels in the smallest convex polygon that can contain the area of a bean seed.\\n8.) Equivalent diameter (Ed): The diameter of a circle having the same area as a bean seed area.\\n9.) Extent (Ex): The ratio of the pixels in the bounding box to the bean area.\\n10.)Solidity (S): Also known as convexity. The ratio of the pixels in the convex shell to those found in beans.\\n11.)Roundness (R): Calculated with the following formula: (4piA)/(P^2)\\n12.)Compactness (CO): Measures the roundness of an object: Ed/L\\n13.)ShapeFactor1 (SF1)\\n14.)ShapeFactor2 (SF2)\\n15.)ShapeFactor3 (SF3)\\n16.)ShapeFactor4 (SF4)\\n17.)Class (Seker, Barbunya, Bombay, Cali, Dermosan, Horoz and Sira)\\n'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#column information and dataset taken from\n",
    "#https://archive.ics.uci.edu/dataset/602/dry+bean+dataset\n",
    "#column information\n",
    "\"\"\"\n",
    "1.) Area (A): The area of a bean zone and the number of pixels within its boundaries.\n",
    "2.) Perimeter (P): Bean circumference is defined as the length of its border.\n",
    "3.) Major axis length (L): The distance between the ends of the longest line that can be drawn from a bean.\n",
    "4.) Minor axis length (l): The longest line that can be drawn from the bean while standing perpendicular to the main axis.\n",
    "5.) Aspect ratio (K): Defines the relationship between L and l.\n",
    "6.) Eccentricity (Ec): Eccentricity of the ellipse having the same moments as the region.\n",
    "7.) Convex area (C): Number of pixels in the smallest convex polygon that can contain the area of a bean seed.\n",
    "8.) Equivalent diameter (Ed): The diameter of a circle having the same area as a bean seed area.\n",
    "9.) Extent (Ex): The ratio of the pixels in the bounding box to the bean area.\n",
    "10.)Solidity (S): Also known as convexity. The ratio of the pixels in the convex shell to those found in beans.\n",
    "11.)Roundness (R): Calculated with the following formula: (4piA)/(P^2)\n",
    "12.)Compactness (CO): Measures the roundness of an object: Ed/L\n",
    "13.)ShapeFactor1 (SF1)\n",
    "14.)ShapeFactor2 (SF2)\n",
    "15.)ShapeFactor3 (SF3)\n",
    "16.)ShapeFactor4 (SF4)\n",
    "17.)Class (Seker, Barbunya, Bombay, Cali, Dermosan, Horoz and Sira)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f368f373-a917-4374-98b2-645c52be7fcd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Class\n",
       "DERMASON    0.260525\n",
       "SIRA        0.193667\n",
       "SEKER       0.148924\n",
       "HOROZ       0.141650\n",
       "CALI        0.119756\n",
       "BARBUNYA    0.097127\n",
       "BOMBAY      0.038351\n",
       "Name: proportion, dtype: float64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bean_df['Class'].value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fec2fed3-886e-41fd-b914-152c5b3b476e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#dermason is the most present and will become the taget class.\n",
    "#apply a lambda function to the target class, creating a new column that\n",
    "#tells us if each instance is or isn't a 'DERMASON' bean or isn't\n",
    "#via 0 if it is not and 1 if it is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "54240815-b41f-4d89-85ff-d54a555d3464",
   "metadata": {},
   "outputs": [],
   "source": [
    "bean_df['Dermason?']= bean_df['Class'].apply(lambda x: 1 if x == 'DERMASON' else 0) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6403f7ef-f7a8-4b71-a5d5-2bf0c5ad69e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Area</th>\n",
       "      <th>Perimeter</th>\n",
       "      <th>MajorAxisLength</th>\n",
       "      <th>MinorAxisLength</th>\n",
       "      <th>AspectRation</th>\n",
       "      <th>Eccentricity</th>\n",
       "      <th>ConvexArea</th>\n",
       "      <th>EquivDiameter</th>\n",
       "      <th>Extent</th>\n",
       "      <th>Solidity</th>\n",
       "      <th>roundness</th>\n",
       "      <th>Compactness</th>\n",
       "      <th>ShapeFactor1</th>\n",
       "      <th>ShapeFactor2</th>\n",
       "      <th>ShapeFactor3</th>\n",
       "      <th>ShapeFactor4</th>\n",
       "      <th>Class</th>\n",
       "      <th>Dermason?</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>28395</td>\n",
       "      <td>610.291</td>\n",
       "      <td>208.178117</td>\n",
       "      <td>173.888747</td>\n",
       "      <td>1.197191</td>\n",
       "      <td>0.549812</td>\n",
       "      <td>28715</td>\n",
       "      <td>190.141097</td>\n",
       "      <td>0.763923</td>\n",
       "      <td>0.988856</td>\n",
       "      <td>0.958027</td>\n",
       "      <td>0.913358</td>\n",
       "      <td>0.007332</td>\n",
       "      <td>0.003147</td>\n",
       "      <td>0.834222</td>\n",
       "      <td>0.998724</td>\n",
       "      <td>SEKER</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>28734</td>\n",
       "      <td>638.018</td>\n",
       "      <td>200.524796</td>\n",
       "      <td>182.734419</td>\n",
       "      <td>1.097356</td>\n",
       "      <td>0.411785</td>\n",
       "      <td>29172</td>\n",
       "      <td>191.272750</td>\n",
       "      <td>0.783968</td>\n",
       "      <td>0.984986</td>\n",
       "      <td>0.887034</td>\n",
       "      <td>0.953861</td>\n",
       "      <td>0.006979</td>\n",
       "      <td>0.003564</td>\n",
       "      <td>0.909851</td>\n",
       "      <td>0.998430</td>\n",
       "      <td>SEKER</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>29380</td>\n",
       "      <td>624.110</td>\n",
       "      <td>212.826130</td>\n",
       "      <td>175.931143</td>\n",
       "      <td>1.209713</td>\n",
       "      <td>0.562727</td>\n",
       "      <td>29690</td>\n",
       "      <td>193.410904</td>\n",
       "      <td>0.778113</td>\n",
       "      <td>0.989559</td>\n",
       "      <td>0.947849</td>\n",
       "      <td>0.908774</td>\n",
       "      <td>0.007244</td>\n",
       "      <td>0.003048</td>\n",
       "      <td>0.825871</td>\n",
       "      <td>0.999066</td>\n",
       "      <td>SEKER</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>30008</td>\n",
       "      <td>645.884</td>\n",
       "      <td>210.557999</td>\n",
       "      <td>182.516516</td>\n",
       "      <td>1.153638</td>\n",
       "      <td>0.498616</td>\n",
       "      <td>30724</td>\n",
       "      <td>195.467062</td>\n",
       "      <td>0.782681</td>\n",
       "      <td>0.976696</td>\n",
       "      <td>0.903936</td>\n",
       "      <td>0.928329</td>\n",
       "      <td>0.007017</td>\n",
       "      <td>0.003215</td>\n",
       "      <td>0.861794</td>\n",
       "      <td>0.994199</td>\n",
       "      <td>SEKER</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>30140</td>\n",
       "      <td>620.134</td>\n",
       "      <td>201.847882</td>\n",
       "      <td>190.279279</td>\n",
       "      <td>1.060798</td>\n",
       "      <td>0.333680</td>\n",
       "      <td>30417</td>\n",
       "      <td>195.896503</td>\n",
       "      <td>0.773098</td>\n",
       "      <td>0.990893</td>\n",
       "      <td>0.984877</td>\n",
       "      <td>0.970516</td>\n",
       "      <td>0.006697</td>\n",
       "      <td>0.003665</td>\n",
       "      <td>0.941900</td>\n",
       "      <td>0.999166</td>\n",
       "      <td>SEKER</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Area  Perimeter  MajorAxisLength  MinorAxisLength  AspectRation  \\\n",
       "0  28395    610.291       208.178117       173.888747      1.197191   \n",
       "1  28734    638.018       200.524796       182.734419      1.097356   \n",
       "2  29380    624.110       212.826130       175.931143      1.209713   \n",
       "3  30008    645.884       210.557999       182.516516      1.153638   \n",
       "4  30140    620.134       201.847882       190.279279      1.060798   \n",
       "\n",
       "   Eccentricity  ConvexArea  EquivDiameter    Extent  Solidity  roundness  \\\n",
       "0      0.549812       28715     190.141097  0.763923  0.988856   0.958027   \n",
       "1      0.411785       29172     191.272750  0.783968  0.984986   0.887034   \n",
       "2      0.562727       29690     193.410904  0.778113  0.989559   0.947849   \n",
       "3      0.498616       30724     195.467062  0.782681  0.976696   0.903936   \n",
       "4      0.333680       30417     195.896503  0.773098  0.990893   0.984877   \n",
       "\n",
       "   Compactness  ShapeFactor1  ShapeFactor2  ShapeFactor3  ShapeFactor4  Class  \\\n",
       "0     0.913358      0.007332      0.003147      0.834222      0.998724  SEKER   \n",
       "1     0.953861      0.006979      0.003564      0.909851      0.998430  SEKER   \n",
       "2     0.908774      0.007244      0.003048      0.825871      0.999066  SEKER   \n",
       "3     0.928329      0.007017      0.003215      0.861794      0.994199  SEKER   \n",
       "4     0.970516      0.006697      0.003665      0.941900      0.999166  SEKER   \n",
       "\n",
       "   Dermason?  \n",
       "0          0  \n",
       "1          0  \n",
       "2          0  \n",
       "3          0  \n",
       "4          0  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bean_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6871bcac-676c-490c-b860-6fa574de3adf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dermason?\n",
       "0    0.739475\n",
       "1    0.260525\n",
       "Name: proportion, dtype: float64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bean_df['Dermason?'].value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "87af3bc9-cd90-47f2-8902-9e8997efd79e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import some relative libraries\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, roc_curve, roc_auc_score, auc\n",
    "from numbers import Number\n",
    "from sklearn.tree import DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e3615848-2bc0-44a1-9607-c015af6507fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#split the data up using train test split\n",
    "X= bean_df.drop(['Dermason?','Class'], axis = 1)\n",
    "y= bean_df['Dermason?']\n",
    "\n",
    "X_train, X_test, y_train, y_test= train_test_split(X, y, random_state= 24, test_size= 0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c486c087-f753-4326-94a5-cfa94e63fb57",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>DecisionTreeClassifier(random_state=24)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">DecisionTreeClassifier</label><div class=\"sk-toggleable__content\"><pre>DecisionTreeClassifier(random_state=24)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "DecisionTreeClassifier(random_state=24)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#start out with a baseline decision tree classifier\n",
    "#train the classifier\n",
    "dt1 = DecisionTreeClassifier(random_state= 24) #dt1 is decision tree #1\n",
    "dt1.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4bea67ba-4da9-496d-874e-cf40eb558921",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9213740607197647"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#make predictions on the test set using the base decision tree classifier\n",
    "#use AUC to calculate predictive performance\n",
    "y_pred_dt1= dt1.predict(X_test)\n",
    "\n",
    "false_positive_rate, true_positive_rate, thresholds = roc_curve(y_test, y_pred_dt1)\n",
    "roc_auc_dt1 = auc(false_positive_rate, true_positive_rate)\n",
    "roc_auc_dt1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "11eeaf55-4aac-4823-947d-8208666f42e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#now let's compare this model to a baseline logidtic regresison model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ca146673-a98c-4fe5-9b20-a9a0d17d0440",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d80e39b5-243a-4354-b07e-30fc6abb9e82",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-2 {color: black;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LogisticRegression(C=1e+16, fit_intercept=False, random_state=24,\n",
       "                   solver=&#x27;liblinear&#x27;)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" checked><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression(C=1e+16, fit_intercept=False, random_state=24,\n",
       "                   solver=&#x27;liblinear&#x27;)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "LogisticRegression(C=1e+16, fit_intercept=False, random_state=24,\n",
       "                   solver='liblinear')"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#lr1 is logistic regression #1\n",
    "lr1 = LogisticRegression(fit_intercept=False, random_state= 24, C=1e16, solver='liblinear')\n",
    "lr1.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d49273ad-df02-404f-843b-16ffd1ab6077",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8825490526583217"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#make predictions on the test set using the base decision tree classifier\n",
    "#use AUC to calculate predictive performance\n",
    "y_pred_lr1= lr1.predict(X_test)\n",
    "\n",
    "false_positive_rate, true_positive_rate, thresholds = roc_curve(y_test, y_pred_lr1)\n",
    "roc_auc_lr1 = auc(false_positive_rate, true_positive_rate)\n",
    "roc_auc_lr1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3e16d22e-b3b4-44a1-90b1-102749ae1292",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\nother possible 1 vs all regressions:\\nNaive bayes\\nRandom Forest\\nK-Nearest Neighbors\\n\\n\\n'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "\n",
    "other possible 1 vs all regressions:\n",
    "Naive bayes\n",
    "Random Forest\n",
    "K-Nearest Neighbors\n",
    "\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7443e67a-0e08-4875-94ba-66f6ba916b72",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9916985460088529"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#lets try a random forest classifier base model\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "rf1= RandomForestClassifier(random_state= 24) #rf1 is random forest #1\n",
    "rf1.fit(X_train, y_train)\n",
    "\n",
    "y_pred_rf1 = rf1.predict_proba(X_test)[:, 1]\n",
    "\n",
    "false_positive_rate, true_positive_rate, thresholds = roc_curve(y_test, y_pred_rf1)\n",
    "roc_auc_rf1 = auc(false_positive_rate, true_positive_rate)\n",
    "roc_auc_rf1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b2bd2156-8b79-439d-a0f3-b5792b4536dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8918746763208552"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#lets try a K-Nearest Neighbors classifier base model\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "\n",
    "knn1= KNeighborsRegressor(n_neighbors=5)\n",
    "knn1.fit(X_train, y_train)\n",
    "\n",
    "y_pred_knn1= knn1.predict(X_test) \n",
    "\n",
    "# Step 6: Convert regression output to binary classification (using a threshold, e.g., median of y_test)\n",
    "threshold = np.median(y_test)  # You could choose a custom threshold as well\n",
    "y_pred_class_knn1 = (y_pred_knn1 > threshold).astype(int)  # Convert continuous predictions to binary\n",
    "\n",
    "auc_knn1 = roc_auc_score((y_test > threshold).astype(int), y_pred_class_knn1)\n",
    "auc_knn1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6e4ac1a7-d20a-4b7e-be01-e954de9d9a01",
   "metadata": {},
   "outputs": [],
   "source": [
    "#right now decision tree and random forrest have the best auc score but could be dues to overfitting. \n",
    "#let's check these scores with cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d9c6509a-3d2f-479d-9c51-26364bc9634b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score, StratifiedKFold\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.base import clone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "da7c25ad-5a73-418c-8c2d-f35e59364c7f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.111110827061766"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dt1 = DecisionTreeClassifier(random_state= 24)\n",
    "\n",
    "# Use cross_val_score with scoring=\"neg_log_loss\" to evaluate the model\n",
    "# on X_train and y_train\n",
    "dt1_baseline_neg_log_loss_cv = cross_val_score(dt1, X_train, y_train, scoring=\"neg_log_loss\")\n",
    "\n",
    "dt1_baseline_log_loss = -(dt1_baseline_neg_log_loss_cv.mean())\n",
    "dt1_baseline_log_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "bbaca19c-d655-4627-92a1-4cbc7948551e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9.269124677583399"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import log_loss\n",
    "\n",
    "log_loss(y_train, np.zeros(len(y_train)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "dc7f47ad-3810-4717-9e57-e57a35a9dc68",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.17333680048338257"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#lets check out the log loss of the logistic regresison model \n",
    "# Use cross_val_score with scoring=\"neg_log_loss\" to evaluate the model\n",
    "# on X_train and y_train\n",
    "lr1_baseline_neg_log_loss_cv = cross_val_score(lr1, X_train, y_train, scoring=\"neg_log_loss\")\n",
    "\n",
    "lr1_baseline_log_loss = -(lr1_baseline_neg_log_loss_cv.mean())\n",
    "lr1_baseline_log_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5060ce07-e3ba-4ec5-b85c-cfab33ef8e0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Make some cross validation test sets for the logistic regression "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "b49956d4-b508-409c-a402-c7463c4928e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "baseline LR neg log loss score: 0.17333680048338257\n",
      "validation LR neg log loss score: 0.17333680048338257\n"
     ]
    }
   ],
   "source": [
    "# Negative log loss doesn't exist as something we can import,\n",
    "# but we can create it\n",
    "neg_log_loss = make_scorer(log_loss, greater_is_better=False, needs_proba=True)\n",
    "\n",
    "# Instantiate the model (same as previous example)\n",
    "lr_baseline_model = lr1\n",
    "\n",
    "# Create a list to hold the score from each fold\n",
    "kfold_scores = np.ndarray(5)\n",
    "\n",
    "# Use cross_val_score with scoring=\"neg_log_loss\" to evaluate the model\n",
    "# on X_train and y_train\n",
    "lr_baseline_neg_log_loss_cv = cross_val_score(lr_baseline_model, X_train, y_train, scoring=\"neg_log_loss\")\n",
    "\n",
    "# Instantiate a splitter object and loop over its result\n",
    "kfold = StratifiedKFold()\n",
    "for fold, (train_index, val_index) in enumerate(kfold.split(X_train, y_train)):\n",
    "    # Extract train and validation subsets using the provided indices\n",
    "    X_t, X_val = X_train.iloc[train_index], X_train.iloc[val_index]\n",
    "    y_t, y_val = y_train.iloc[train_index], y_train.iloc[val_index]\n",
    "    \n",
    "    # Clone the provided model and fit it on the train subset\n",
    "    temp_model = clone(lr_baseline_model)\n",
    "    temp_model.fit(X_t, y_t)\n",
    "    \n",
    "    # Evaluate the provided model on the validation subset\n",
    "    neg_log_loss_score = neg_log_loss(temp_model, X_val, y_val)\n",
    "    kfold_scores[fold] = neg_log_loss_score\n",
    "    \n",
    "print(f\"baseline LR neg log loss score: {-lr_baseline_neg_log_loss_cv.mean()}\")\n",
    "print(f\"validation LR neg log loss score: {-kfold_scores.mean()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "10c771e8-f8db-49e8-9e13-9d505b268585",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "baseline DT neg log loss score: 2.111110827061766\n",
      "validation DT neg log loss score: 2.111110827061766\n"
     ]
    }
   ],
   "source": [
    "#make a cross validation test set for decision tree\n",
    "# Negative log loss doesn't exist as something we can import,\n",
    "# but we can create it\n",
    "neg_log_loss = make_scorer(log_loss, greater_is_better=False, needs_proba=True)\n",
    "\n",
    "# Instantiate the model (same as previous example)\n",
    "dt_baseline_model = dt1\n",
    "\n",
    "# Create a list to hold the score from each fold\n",
    "kfold_scores = np.ndarray(5)\n",
    "\n",
    "# Use cross_val_score with scoring=\"neg_log_loss\" to evaluate the model\n",
    "# on X_train and y_train\n",
    "dt_baseline_neg_log_loss_cv = cross_val_score(dt_baseline_model, X_train, y_train, scoring=\"neg_log_loss\")\n",
    "\n",
    "# Instantiate a splitter object and loop over its result\n",
    "kfold = StratifiedKFold()\n",
    "for fold, (train_index, val_index) in enumerate(kfold.split(X_train, y_train)):\n",
    "    # Extract train and validation subsets using the provided indices\n",
    "    X_t, X_val = X_train.iloc[train_index], X_train.iloc[val_index]\n",
    "    y_t, y_val = y_train.iloc[train_index], y_train.iloc[val_index]\n",
    "    \n",
    "    # Clone the provided model and fit it on the train subset\n",
    "    temp_model = clone(dt_baseline_model)\n",
    "    temp_model.fit(X_t, y_t)\n",
    "    \n",
    "    # Evaluate the provided model on the validation subset\n",
    "    neg_log_loss_score = neg_log_loss(temp_model, X_val, y_val)\n",
    "    kfold_scores[fold] = neg_log_loss_score\n",
    "    \n",
    "print(f\"baseline DT neg log loss score: {-dt_baseline_neg_log_loss_cv.mean()}\")\n",
    "print(f\"validation DT neg log loss score: {-kfold_scores.mean()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "26f9183f-dda8-45ca-9af1-885c28e4b6ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "#lets mess around with some preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "2343261d-1c3a-48f9-98c1-128e1ec2d118",
   "metadata": {},
   "outputs": [],
   "source": [
    "#start out with stanbdard scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "230a3c98-3727-45c1-bd9d-fe879e7510d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#logistic regression "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "7e2e340e-7cce-4d4c-a7a5-8d61d41428f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "baseline LR neg log loss score: 0.17333680048338257\n",
      "preprocessed validation LR neg log loss score: 0.17498147673509248\n"
     ]
    }
   ],
   "source": [
    "# Import relevant sklearn and imblearn classes\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "def custom_cross_val_score(estimator, X, y):\n",
    "    # Create a list to hold the scores from each fold\n",
    "    kfold_train_scores = np.ndarray(5)\n",
    "    kfold_val_scores = np.ndarray(5)\n",
    "\n",
    "    # Instantiate a splitter object and loop over its result\n",
    "    kfold = StratifiedKFold(n_splits=5)\n",
    "    for fold, (train_index, val_index) in enumerate(kfold.split(X, y)):\n",
    "        # Extract train and validation subsets using the provided indices\n",
    "        X_t, X_val = X.iloc[train_index], X.iloc[val_index]\n",
    "        y_t, y_val = y.iloc[train_index], y.iloc[val_index]\n",
    "        \n",
    "        # Instantiate StandardScaler\n",
    "        scaler = StandardScaler()\n",
    "        # Fit and transform X_t\n",
    "        X_t_scaled = scaler.fit_transform(X_t)\n",
    "        # Transform X_val\n",
    "        X_val_scaled = scaler.transform(X_val)\n",
    "        \n",
    "        # Clone the provided model and fit it on the train subset\n",
    "        temp_model = clone(estimator)\n",
    "        temp_model.fit(X_t_scaled, y_t)\n",
    "        \n",
    "        # Evaluate the provided model on the train and validation subsets\n",
    "        neg_log_loss_score_train = neg_log_loss(temp_model, X_t_scaled, y_t)\n",
    "        neg_log_loss_score_val = neg_log_loss(temp_model, X_val_scaled, y_val)\n",
    "        kfold_train_scores[fold] = neg_log_loss_score_train\n",
    "        kfold_val_scores[fold] = neg_log_loss_score_val\n",
    "        \n",
    "    return kfold_train_scores, kfold_val_scores\n",
    "\n",
    "model_with_preprocessing = LogisticRegression(fit_intercept=False, random_state= 24, C=1e16, solver='liblinear')\n",
    "preprocessed_train_scores, preprocessed_neg_log_loss_cv = custom_cross_val_score(model_with_preprocessing, X_train, y_train)\n",
    "\n",
    "print(f\"baseline LR neg log loss score: {-lr_baseline_neg_log_loss_cv.mean()}\")\n",
    "print(f\"preprocessed validation LR neg log loss score: {-preprocessed_neg_log_loss_cv.mean()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "c4be763a-1c44-4cd5-af19-0d551aa3e87f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#on the logistic regression is seems the standardization has reulted in the classification worsening\n",
    "#this points to overfitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "04625443-ff67-4a54-9842-db1c83bc2dbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "baseline DT neg log loss score: 2.111110827061766\n",
      "preprocessed validation DT neg log loss score: 2.111110827061766\n"
     ]
    }
   ],
   "source": [
    "#now try it with the decision tree\n",
    "model_with_preprocessing = DecisionTreeClassifier(random_state= 24)\n",
    "preprocessed_train_scores, preprocessed_neg_log_loss_cv = custom_cross_val_score(model_with_preprocessing, X_train, y_train)\n",
    "\n",
    "print(f\"baseline DT neg log loss score: {-dt_baseline_neg_log_loss_cv.mean()}\")\n",
    "print(f\"preprocessed validation DT neg log loss score: {-preprocessed_neg_log_loss_cv.mean()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "05c69033-1614-43e8-86bb-4a07a83c395b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#normalization does not seem to affect the decision tree as hypothesized"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a89c539-e5f7-4fd6-b4f4-ef3475a81fc1",
   "metadata": {},
   "source": [
    "# let's run it again but this time use specificity as the rate for success"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "d53600b2-27b4-486f-9293-f89bf8e505d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "f191eb66-1871-47b8-b80d-8cf0a4f9b22f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#split the data up using train test split\n",
    "X= bean_df.drop(['Dermason?','Class'], axis = 1)\n",
    "y= bean_df['Dermason?']\n",
    "\n",
    "X_train, X_test, y_train, y_test= train_test_split(X, y, random_state= 24, test_size= 0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "1f08d981-b11a-4601-a082-f010914e33d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix:\n",
      "[[2878  110]\n",
      " [ 132  964]]\n",
      "Specificity: 0.963186\n"
     ]
    }
   ],
   "source": [
    "#decision Tree base model\n",
    "\n",
    "#start out with a baseline decision tree classifier\n",
    "#train the classifier\n",
    "dt1 = DecisionTreeClassifier(random_state= 24) #dt1 is decision tree #1\n",
    "dt1.fit(X_train, y_train)\n",
    "\n",
    "#make predictions on the test set using the base decision tree classifier\n",
    "#use AUC to calculate predictive performance\n",
    "y_pred_dt1= dt1.predict(X_test)\n",
    "\n",
    "#make the confusion matrix\n",
    "dt1_cm= confusion_matrix(y_test, y_pred_dt1)\n",
    "\n",
    "#extract the prediction scores from the confusion matrix\n",
    "TN, FP, FN, TP= dt1_cm.ravel()\n",
    "\n",
    "#calculate the specificity (true negative rate)\n",
    "dt1_specificity = TN / (TN + FP)\n",
    "\n",
    "# Print the confusion matrix and specificity rounded to 4 decimals\n",
    "print(\"Confusion Matrix:\")\n",
    "print(dt1_cm)\n",
    "print(f\"Specificity: {dt1_specificity:.6}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "c20beffe-f14e-441c-91d2-0f11a76692f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix:\n",
      "[[2845  143]\n",
      " [ 205  891]]\n",
      "Specificity: 0.952142\n"
     ]
    }
   ],
   "source": [
    "#logistic regression base model\n",
    "\n",
    "\n",
    "lr1 = LogisticRegression(fit_intercept=False, random_state= 24, C=1e16, solver='liblinear')\n",
    "lr1.fit(X_train, y_train)\n",
    "\n",
    "#make predictions on the test set using the base decision tree classifier\n",
    "#use AUC to calculate predictive performance\n",
    "y_pred_lr1= lr1.predict(X_test)\n",
    "\n",
    "#make the confusion matrix\n",
    "lr1_cm= confusion_matrix(y_test, y_pred_lr1)\n",
    "\n",
    "#extract the prediction scores from the confusion matrix\n",
    "TN, FP, FN, TP= lr1_cm.ravel()\n",
    "\n",
    "#calculate the specificity (true negative rate)\n",
    "lr1_specificity = TN / (TN + FP)\n",
    "\n",
    "# Print the confusion matrix and specificity rounded to 4 decimals\n",
    "print(\"Confusion Matrix:\")\n",
    "print(lr1_cm)\n",
    "print(f\"Specificity: {lr1_specificity:.6}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bbc14e2-ecb2-483e-8a64-195c8d23681c",
   "metadata": {},
   "source": [
    "# make the cross validation models for specificity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "b76cd41c-0897-4971-b0c3-e79994832c96",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import make_scorer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91cceacc-cfdb-436e-8ce3-16856dda8683",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define a function to calculate specificity from confusion matrix\n",
    "def specificity_score(y_true, y_pred):\n",
    "    cm = confusion_matrix(y_true, (y_pred > 0.5).astype(int))\n",
    "    TN = cm[0, 0]  # True Negatives\n",
    "    FP = cm[0, 1]  # False Positives\n",
    "    return TN / (TN + FP)\n",
    "\n",
    "#Create a custom scorer using make_scorer for specificity\n",
    "specificity_scorer = make_scorer(specificity_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "c1376b10-4dd9-4f59-a044-27491a80a09c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "baseline LR specificity score: 0.9559124393603641\n",
      "validation LR specificity score: 0.9559124393603641\n"
     ]
    }
   ],
   "source": [
    "#logistic regression cross validation\n",
    "\n",
    "# Instantiate the model (same as previous example)\n",
    "lr_baseline_model = LogisticRegression(fit_intercept=False, random_state= 24, C=1e16, solver='liblinear')\n",
    "\n",
    "# Create a list to hold the score from each fold\n",
    "kfold_scores = np.ndarray(5)\n",
    "\n",
    "# Use cross_val_score with scoring=\"neg_log_loss\" to evaluate the model\n",
    "# on X_train and y_train\n",
    "lr_baseline_specificity_cv = cross_val_score(lr_baseline_model, X_train, y_train, scoring= specificity_scorer)\n",
    "\n",
    "# Instantiate a splitter object and loop over its result\n",
    "kfold = StratifiedKFold()\n",
    "for fold, (train_index, val_index) in enumerate(kfold.split(X_train, y_train)):\n",
    "    # Extract train and validation subsets using the provided indices\n",
    "    X_t, X_val = X_train.iloc[train_index], X_train.iloc[val_index]\n",
    "    y_t, y_val = y_train.iloc[train_index], y_train.iloc[val_index]\n",
    "    \n",
    "    # Clone the provided model and fit it on the train subset\n",
    "    temp_model = clone(lr_baseline_model)\n",
    "    temp_model.fit(X_t, y_t)\n",
    "\n",
    "    # Predict probabilities on the validation set\n",
    "    y_pred_proba = temp_model.predict_proba(X_val)[:, 1]\n",
    "\n",
    "    # Calculate specificity score for the fold\n",
    "    specificity = specificity_score(y_val, y_pred_proba)\n",
    "    kfold_scores[fold] = specificity\n",
    "\n",
    "print(f\"baseline LR specificity score: {lr_baseline_specificity_cv.mean()}\")\n",
    "print(f\"validation LR specificity score: {kfold_scores.mean()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "70d32e37-45da-4148-8238-7c120d12910f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "baseline DT specificity score: 0.9608585374618194\n",
      "validation DT specificity score: 0.9608585374618194\n"
     ]
    }
   ],
   "source": [
    "#decision tree cross validation\n",
    "\n",
    "# Instantiate the model (same as previous example)\n",
    "dt_baseline_model = DecisionTreeClassifier(random_state= 24)\n",
    "\n",
    "# Create a list to hold the score from each fold\n",
    "kfold_scores = np.ndarray(5)\n",
    "\n",
    "# Use cross_val_score with scoring=\"neg_log_loss\" to evaluate the model\n",
    "# on X_train and y_train\n",
    "dt_baseline_specificity_cv = cross_val_score(dt_baseline_model, X_train, y_train, scoring= specificity_scorer)\n",
    "\n",
    "# Instantiate a splitter object and loop over its result\n",
    "kfold = StratifiedKFold()\n",
    "for fold, (train_index, val_index) in enumerate(kfold.split(X_train, y_train)):\n",
    "    # Extract train and validation subsets using the provided indices\n",
    "    X_t, X_val = X_train.iloc[train_index], X_train.iloc[val_index]\n",
    "    y_t, y_val = y_train.iloc[train_index], y_train.iloc[val_index]\n",
    "    \n",
    "    # Clone the provided model and fit it on the train subset\n",
    "    temp_model = clone(dt_baseline_model)\n",
    "    temp_model.fit(X_t, y_t)\n",
    "\n",
    "    # Predict probabilities on the validation set\n",
    "    y_pred_proba = temp_model.predict_proba(X_val)[:, 1]\n",
    "\n",
    "    # Calculate specificity score for the fold\n",
    "    specificity = specificity_score(y_val, y_pred_proba)\n",
    "    kfold_scores[fold] = specificity\n",
    "\n",
    "print(f\"baseline DT specificity score: {dt_baseline_specificity_cv.mean()}\")\n",
    "print(f\"validation DT specificity score: {kfold_scores.mean()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "140b8526-83c2-4f53-81fa-968ea6514f79",
   "metadata": {},
   "source": [
    "# specificity preprocessing function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "c8ae3daf-48e3-46df-ab93-386979ce4350",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "baseline LR neg log loss score: 0.17333680048338257\n",
      "preprocessed validation LR neg log loss score: 0.17498147673509248\n"
     ]
    }
   ],
   "source": [
    "# Import relevant sklearn and imblearn classes\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "def custom_cross_val_score(estimator, X, y):\n",
    "    # Create a list to hold the scores from each fold\n",
    "    kfold_train_scores = np.ndarray(5)\n",
    "    kfold_val_scores = np.ndarray(5)\n",
    "\n",
    "    # Instantiate a splitter object and loop over its result\n",
    "    kfold = StratifiedKFold(n_splits=5)\n",
    "    for fold, (train_index, val_index) in enumerate(kfold.split(X, y)):\n",
    "        # Extract train and validation subsets using the provided indices\n",
    "        X_t, X_val = X.iloc[train_index], X.iloc[val_index]\n",
    "        y_t, y_val = y.iloc[train_index], y.iloc[val_index]\n",
    "        \n",
    "        # Instantiate StandardScaler\n",
    "        scaler = StandardScaler()\n",
    "        # Fit and transform X_t\n",
    "        X_t_scaled = scaler.fit_transform(X_t)\n",
    "        # Transform X_val\n",
    "        X_val_scaled = scaler.transform(X_val)\n",
    "        \n",
    "        # Clone the provided model and fit it on the train subset\n",
    "        temp_model = clone(estimator)\n",
    "        temp_model.fit(X_t_scaled, y_t)\n",
    "        \n",
    "        # Evaluate the provided model on the train and validation subsets\n",
    "        neg_log_loss_score_train = neg_log_loss(temp_model, X_t_scaled, y_t)\n",
    "        neg_log_loss_score_val = neg_log_loss(temp_model, X_val_scaled, y_val)\n",
    "        kfold_train_scores[fold] = neg_log_loss_score_train\n",
    "        kfold_val_scores[fold] = neg_log_loss_score_val\n",
    "        \n",
    "    return kfold_train_scores, kfold_val_scores\n",
    "\n",
    "model_with_preprocessing = LogisticRegression(fit_intercept=False, random_state= 24, C=1e16, solver='liblinear')\n",
    "preprocessed_train_scores, preprocessed_neg_log_loss_cv = custom_cross_val_score(model_with_preprocessing, X_train, y_train)\n",
    "\n",
    "print(f\"baseline LR neg log loss score: {-lr_baseline_neg_log_loss_cv.mean()}\")\n",
    "print(f\"preprocessed validation LR neg log loss score: {-preprocessed_neg_log_loss_cv.mean()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b61aa9ca-b8ef-4b32-9be7-c2780da2a4d2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
